# ğŸŒŸâš¡ **Lumina**

<p align="center">
  <img src="assets/lumina-banner.png" alt="Lumina - Document Intelligence Platform" width="600"/>
</p>

### **From Unstructured Chaos to Structured Insight | Multi-Agent Document Intelligence**

[![FastAPI](https://img.shields.io/badge/FastAPI-0.104-009688?logo=fastapi&logoColor=white)]()
[![Python](https://img.shields.io/badge/Python-3.9+-blue?logo=python&logoColor=white)]()
[![NVIDIA](https://img.shields.io/badge/NVIDIA-NIM-76B900?logo=nvidia&logoColor=white)]()
[![Google](https://img.shields.io/badge/Google-Gemini-4285F4?logo=google&logoColor=white)]()
[![React](https://img.shields.io/badge/Frontend-React-61DAFB?logo=react&logoColor=white)]()

---

## âš¡ Quick Start

Get Lumina running in 5 minutes:

```bash
# 1. Clone the repository
git clone https://github.com/Bourn23/doc2table.git
cd doc2table

# 2. Setup environment and credentials (one-time setup)
chmod +x setup-environment.sh
./setup-environment.sh interactive

# 3. Run the deployment script
chmod +x manage-lumina.sh
./manage-lumina.sh

# 4. Select Option 2: Deploy Backend
# 5. Select Option 3: Deploy Frontend
```

**That's it!** Your Lumina instance will be live at the S3 URL shown by the script.

ğŸ“– **New to Lumina?** Check out the [Deployment Guide](#-deployment-guide) for detailed instructions.

---

## ğŸ“š Table of Contents

- [Overview](#-overview)
- [The Problem](#the-problem-with-manual-data-extraction)
- [Our Solution](#our-solution-agentic-document-intelligence)
- [Core Stack](#-core-stack)
- [Features](#-features)
- [**ğŸš€ Deployment Guide**](#-deployment-guide) â† **Start Here**
- [Technology Stack Details](#-technology-stack-details)
- [Project Structure](#ï¸-project-structure)
- [Roadmap](#ï¸-roadmap)
- [Contributing](#-contributing)

---

## ğŸ“– Additional Documentation

- **[Technical Deep Dive](TECHNICAL_DEEP_DIVE.md)** - Comprehensive guide to architecture, AI integration, and implementation details
- **[Hackathon Evaluation](kiro-documentation/hackathon-evaluation/)** - Detailed technical analysis and competitive positioning

---

## ğŸ¯ Overview

**Lumina** is an **intelligent document processing system** that transforms unstructured documents into queryable, structured knowledge. Built on a **multi-agent AI architecture**, it automatically analyzes your documents, designs optimal extraction schemas, extracts data with precision, and enables natural language querying through advanced RAG (Retrieval-Augmented Generation).

> ğŸ”® *Lumina combines makes document intelligence automated, precise, and conversational. And above all, open-sourced.*

---

## The Problem: Document Analysis Doesn't Scale

**Have you ever needed to analyze dozens of contracts, research papers, or financial reportsâ€”only to hit the '10 file upload limit' on every AI tool?**

### Current Reality
- **Manual data extraction costs U.S. companies $28,500 per employee annually**
- Researchers spend **28% of their time** manually extracting data from papers
- Existing AI tools (ChatGPT, Claude) limit you to **10-20 files maximum**
- Academic tools (SciSpace, Elicit) only work with research papers, not business documents
- Enterprise solutions (Hebbia, Palantir) cost **$10,000+ per seat annually**
- **No open-source alternatives** exist that combine scale, validation, and conversational queries

### The Traditional Workflow
1. **Manually design** schemas for each document type
2. **Labor-intensive** reading of all documents
3. **Manually extract** data from PDFs into spreadsheets
4. **Re-read** documents multiple times to answer new questions
5. **Lose context** when documents don't fit your predefined structure

**This is slow, error-prone, and doesn't scale** when you have 20+ documents to analyze.


## Our Solution: Agentic AI for Document Intelligence

We fix this by using **specialized AI agents** that collaborates with the user, understand your documents, design extraction schemas, extract structured data, and answer questions.

### What is Agentic Extraction?

Instead of one large AI model trying to do everything, Lumina uses **seven specialized agents** working together:

- **Document Classifier**: Analyzes document types, domains, and content structure
- **Intention Recommender**: Suggests what data you should extract based on document analysis
- **Schema Designer**: Creates optimized Pydantic models for your extraction task
- **Pydantic Code Generator**: Writes production-ready validation code
- **Extraction Agent**: Extracts structured data from each document in parallel
- **Query Router**: Classifies user questions and routes to the right processing pipeline
- **Answer Synthesizer**: Generates cited answers from retrieved evidence

Unlike single-model systems (which hallucinate), our **agent pipeline is deterministic** â€” each agent has one job and does it well.

![Lumina Workflow](image-1.png)
---

**Similar Services:**
| Feature | Lumina | ChatGPT/Claude | SciSpace/Elicit | Hebbia/Palantir |
|---------|--------|----------------|-----------------|-----------------|
| File Limit | Unlimited | 10-20 | Research only | Unlimited |
| Validation | âœ… Pydantic | âŒ None | âŒ None | âœ… Custom |
| Open Source | âœ… Yes | âŒ No | âŒ No | âŒ No |
| Cost | ~$32/mo | $20/mo | $20/mo | $10K+/year |
| Custom Schema | âœ… Yes | âŒ No | âŒ No | âœ… Yes |
| Citation | âœ… Yes | âš ï¸ Limited | âœ… Yes | âœ… Yes |

## ğŸ§© Core Stack

| Layer | Technology | Purpose |
|-------|-------------|---------|
| **Orchestration** | `OpenAI Agents SDK` | Multi-agent coordination and tool calling |
| **Generation** | `NVIDIA llama-3.1-nemotron-8b` | High-quality text generation for extraction |
| **Embedding** | `NVIDIA llama-3.2-nv-retriever-300m` | Vector embeddings for semantic search |
| **Reranking** | `NVIDIA llama-3.2-nemoretriever-reranker-500m` | Precision reranking of search results |
| **Vector Compute** | `FAISS` | GPU-accelerated similarity search |
| **API Framework** | `FastAPI (Async)` | High-performance async REST API |
| **Database** | `SQLAlchemy (Async) + PostgreSQL/SQLite` | Persistent session and record storage |
| **Knowledge Graph** | `LangChain` | Relationship extraction |
| **Document Processing** | `PyMuPDF (fitz) + pandas` | PDF, CSV, text file parsing |
| **Schema Validation** | `Pydantic V2` | Dynamic model generation with validators |
| **Frontend (UI)** | `React` | Interactive document upload and query interface |

---

## ğŸš€ Features

### Core Capabilities
- ğŸ¤– **7 Specialized Agents** â€“ Each with a focused task in the extraction pipeline
- ğŸ“„ **Multi-Format Processing** â€“ Handles various types of documents including PDFs, CSVs, text files, markdown
- ğŸ§  **Intelligent Schema Generation** â€“ AI recommends optimal extraction fields
- âš¡ **Parallel Extraction** â€“ Concurrent processing of multiple documents
- ğŸ” **Advanced RAG Pipeline** â€“ Embedding â†’ Retrieval â†’ Reranking â†’ Generation
- ğŸ¯ **Dynamic Field Extraction** â€“ Add new fields without reprocessing documents
- ğŸ—ºï¸ **Knowledge Graph Construction** â€“ Automatic relationship discovery
- ğŸ’¾ **Production-Ready** â€“ Async database, retry logic, error handling

### What Makes Lumina Different
- âœ… **User-in-the-Loop Schema Design** â€“ Review, customize, and edit before extraction
- âœ… **Validation at Every Step** â€“ Code validation, structure checks, retry logic
- âœ… **Citation-Backed Answers** â€“ Every fact traced to source document
- âœ… **Markdown Caching** â€“ Converted documents cached for re-use
- âœ… **Export to CSV/JSON** â€“ Download structured data with timestamps

---

### Lumina's Approach:
```
Query: "What temperature was used for TiO2 synthesis?"

1. Router Agent â†’ Detects 'temperature' field exists â†’ rag_row_wise
2. GPU Search â†’ Retrieves records mentioning TiO2
3. Reranker â†’ Re-sorts by relevance
4. Structured Evidence:
   {
     "material_name": "TiO2 nanoparticles",
     "synthesis.temperature": 350.0,
     "_source_document": "paper_1.pdf"
   }
5. Answer Synthesizer â†’ Uses structured field directly
Answer: "TiO2 was synthesized at 350.0Â°C (paper_1.pdf:record_0)" âœ…
```

**Key Differences:**
- âœ… **Structured Fields**: Extraction validates types (number vs string)
- âœ… **Source Tracking**: Every value traced to document + record ID
- âœ… **No Re-Parsing**: LLM doesn't re-interpret text (uses pre-extracted data)
- âœ… **Validation**: Pydantic ensures data quality (ranges, patterns)

---


---

## ğŸ“Š Technology Stack Details

### Multi-Agent Orchestration
- **OpenAI Agents SDK**: Tool calling, structured outputs, agent coordination
- **Retry Logic**: Exponential backoff with timeout for LLM calls

### Embedding & Retrieval
- **NVIDIA NIM Embedder**: `llama-3.2-nemoretriever-300m-embed-v1`
  - 2048-dimensional embeddings
  - Separate encoding for queries vs passages
  - OpenAI-compatible API


### Reranking
- **NVIDIA NIM Reranker**: `llama-3.2-nemoretriever-500m-rerank-v2`
  - Improves precision of top-k results
  - Logit scores for relevance ranking
  - REST API with requests Session

### Generation
- **Gemini 2.5 Flash**: Primary LLM for extraction
   - PyDantic schema generation, extraction
- **NVIDIA llama 3.1 Nemotron 8B**: Primary LLM for query
  - Query router agent
  - Answer synthesis and Knowledge graph generation
  - Structured output with Pydantic models
  - Context window: 1M tokens
- **Ollama (Optional)**: Local model support (coming soon)
  - Qwen 3 0.6B for lightweight extraction
  - Self-hosted alternative to cloud APIs

### Data Storage
- **SQLAlchemy (Async)**:
  - PostgreSQL for production
  - SQLite for development
  - Async sessions with connection pooling
- **Tables**:
  - `sessions`: Extraction jobs with schemas
  - `uploaded_files`: Document metadata
  - `extracted_records`: Structured data (JSONB column)

### Document Processing
- **PyMuPDF (fitz)**: PDF text extraction
  - Page-by-page parsing
  - Handles multi-column layouts
- **pandas**: CSV/Excel processing
- **Markdown Caching**: Converted docs saved to `converted_markdown/`

### Knowledge Graph (Optional)

- **LangChain**: Document â†’ Graph transformation
  - Entity extraction from text
  - Relationship inference

---

## ğŸ—ï¸ Project Structure

```
lumina/
â”œâ”€â”€ lumina-backend/                 # Backend microservices
â”‚   â”œâ”€â”€ api_service/               # API Gateway (port 8000)
â”‚   â”‚   â”œâ”€â”€ main.py               # Orchestrates extraction/query services
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ requirements.txt
â”‚   â”‚
â”‚   â”œâ”€â”€ extraction_service/        # Document processing (port 8001)
â”‚   â”‚   â”œâ”€â”€ main.py               # Schema generation, data extraction
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ converted_markdown/   # Cached converted documents
â”‚   â”‚
â”‚   â”œâ”€â”€ query_service/             # RAG and querying (port 8002)
â”‚   â”‚   â”œâ”€â”€ main.py               # Indexing, semantic search
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ indexes/              # FAISS vector indexes
â”‚   â”‚
â”‚   â”œâ”€â”€ lumina_agents/             # AI agents package (modular)
â”‚   â”‚   â”œâ”€â”€ __init__.py           # Package exports
â”‚   â”‚   â”œâ”€â”€ config.py             # Model configuration & selectors
â”‚   â”‚   â”œâ”€â”€ tools.py              # Agent tool functions
â”‚   â”‚   â”œâ”€â”€ schema_agents.py      # Schema generation agents
â”‚   â”‚   â”œâ”€â”€ extraction_agents.py  # Document extraction pipeline
â”‚   â”‚   â”œâ”€â”€ analysis_agents.py    # Document classification agents
â”‚   â”‚   â”œâ”€â”€ query_agents.py       # Query routing & synthesis
â”‚   â”‚   â””â”€â”€ rag_agent.py          # RAG system implementation
â”‚   â”‚
â”‚   â”œâ”€â”€ shared/                    # Shared utilities
â”‚   â”‚   â”œâ”€â”€ models.py             # SQLAlchemy database models
â”‚   â”‚   â”œâ”€â”€ database.py           # Async DB setup
â”‚   â”‚   â”œâ”€â”€ job_manager.py        # Redis-based job tracking
â”‚   â”‚   â”œâ”€â”€ api_types.py          # Pydantic request/response models
â”‚   â”‚   â”œâ”€â”€ tools.py              # Shared tool definitions
â”‚   â”‚   â”œâ”€â”€ utils.py              # Helper functions
â”‚   â”‚   â””â”€â”€ aws_client.py         # S3 export utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ tests/                     # Test files
â”‚   â””â”€â”€ docker-compose.yml         # Multi-service orchestration
â”‚
â”œâ”€â”€ lumina-frontend-async/         # React frontend
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ App.tsx               # Main app, phase orchestration
â”‚   â”‚   â”œâ”€â”€ components/           # Reusable UI components
â”‚   â”‚   â”œâ”€â”€ phases/               # Phase-specific views
â”‚   â”‚   â”œâ”€â”€ store/                # Zustand global state
â”‚   â”‚   â”œâ”€â”€ types/                # TypeScript definitions
â”‚   â”‚   â””â”€â”€ utils/                # API client, job manager
â”‚   â”‚
â”‚   â”œâ”€â”€ package.json
â”‚   â””â”€â”€ Dockerfile                # Nginx production build
â”‚
â”œâ”€â”€ scripts/                       # Deployment scripts
â”œâ”€â”€ manage-lumina.sh              # Main deployment orchestration
â”œâ”€â”€ .env                          # API keys (not committed)
â””â”€â”€ README.md
```

### Key Architecture Changes

**Modular Agent Package**: The AI agents have been refactored from monolithic files into a clean, modular package structure (`lumina_agents/`):

- **Before**: Two large files (`agents_collection.py` ~950 lines, `rag_agent.py` ~800 lines) with mixed concerns
- **After**: Seven focused modules with clear separation of concerns

**Benefits**:
- âœ… Easier to locate and modify specific agent functionality
- âœ… Better code organization and maintainability
- âœ… Cleaner imports and exports
- âœ… Production-ready with proper type hints and docstrings
- âœ… No duplicate code or commented-out sections

---



---

## ğŸš€ Deployment Guide

### Prerequisites

Before deploying Lumina, ensure you have:

1. **AWS Account** with credentials ready
2. **API Keys**:
   - NVIDIA API Key ([Get it here](https://build.nvidia.com/settings/api-keys))
   - Google Gemini API Key ([Get it here](https://aistudio.google.com/api-keys))
3. **Local Tools**:
   - Node.js 24+ (for frontend)
   - Docker (optional, for local testing)
   - SSH key pair for EC2 (or let the script create one)

### Quick Start (5 Minutes)

The easiest way to deploy Lumina is using the setup and management scripts:

```bash
# Step 1: Setup environment (one-time)
chmod +x setup-environment.sh
./setup-environment.sh interactive

# Step 2: Run the deployment manager
chmod +x manage-lumina.sh
./manage-lumina.sh
```

### Step-by-Step Deployment

#### Step 0: Setup Environment (One-Time)

Before deploying, configure your credentials:

```bash
# Run the interactive setup
./setup-environment.sh interactive
```

This will prompt you for:
- **NVIDIA API Key** - Get from [NVIDIA NGC](https://build.nvidia.com/settings/api-keys) (used for both RAG and KG (Knowledge Graph) services)
- **Google Gemini API Key** - Get from [Google AI Studio](https://aistudio.google.com/api-keys) (used for extraction)
- **AWS Credentials** - Access Key ID, Secret Access Key, Region

All credentials are saved to a `.env` file and automatically used by deployment scripts.

#### Step 1: Deploy Backend to AWS EC2

1. **Run the management script**:
   ```bash
   ./manage-lumina.sh
   ```

2. **Select Option 2: Deploy Backend**
   - This will create a t3.medium EC2 instance (~$30/month)
   - Wait for the instance to be created (2-3 minutes)
   - When prompted, choose to deploy the application
   - **API keys from your .env file are automatically configured!**

3. **Verify Backend is Running**:
   ```bash
   # From the management script, select Option 10: Test Endpoints
   # Or manually test:
   curl http://<your-public-ip>:8000/
   ```

#### Step 2: Deploy Frontend to S3

1. **From the management script, select Option 3: Deploy Frontend**

2. **Choose to build the frontend** when prompted
   - The script will automatically configure the correct backend URL
   - Build the frontend (takes 1-2 minutes)
   - Deploy to S3 (recommended)

3. **Access Your Application**:
   - The script will show you the frontend URL
   - Open it in your browser
   - You should see the Lumina interface

#### Step 3: Test the Full System

1. **Upload a test document**:
   - Click "Upload Documents"
   - Select a PDF or CSV file
   - Wait for analysis to complete

2. **Review and confirm the schema**:
   - Review the AI-recommended fields
   - Edit if needed
   - Click "Confirm and Extract"

3. **Query your data**:
   - Try: "Summarize the data"
   - Try: "Create a new column for [something]"
   - Try: "Export as CSV"

### Managing Your Deployment

The `manage-lumina.sh` script provides all the tools you need:

```
1. ğŸ¤– Deploy NIM Models          - Deploy NVIDIA NIM on EKS (optional)
2. ğŸ—ï¸  Deploy Backend            - Initial backend deployment
3. ğŸŒ Deploy Frontend            - Initial frontend deployment

4. ğŸ“Š Show System Status         - Check if services are running
5. â¸ï¸  Pause System              - Stop EC2 to save costs
6. â–¶ï¸  Resume System             - Restart EC2 instance
7. ğŸ”„ Restart Docker Services    - Restart containers without rebuild
8. ğŸš€ Redeploy Backend Code      - Deploy code changes to backend
9. ğŸŒ Redeploy Frontend          - Deploy code changes to frontend

10. ğŸ§ª Test Endpoints            - Verify all services are responding
11. ğŸ“‹ Show Deployment Info      - Show URLs, IPs, and costs
12. ğŸ“œ View Docker Logs          - Debug backend issues
13. ğŸ”§ Fix Frontend Issues       - Fix API URL mismatches

14. ğŸ§¹ Clean Up All Resources    - Delete everything (cannot be undone)
```

### Common Deployment Scenarios

#### Scenario 1: Code Changes to Backend

```bash
# 1. Make your code changes locally
# 2. Run the management script
./manage-lumina.sh

# 3. Select Option 8: Redeploy Backend Code
# This will:
# - Copy your updated code to EC2
# - Rebuild Docker containers
# - Restart all services
```

#### Scenario 2: Code Changes to Frontend

```bash
# 1. Make your code changes locally
# 2. Run the management script
./manage-lumina.sh

# 3. Select Option 9: Redeploy Frontend
# This will:
# - Fix .env.production
# - Rebuild the frontend
# - Deploy to S3
```

#### Scenario 3: Debugging Issues

```bash
# View logs
./manage-lumina.sh
# Select Option 12: View Docker Logs

# Check system status
./manage-lumina.sh
# Select Option 4: Show System Status

# Test endpoints
./manage-lumina.sh
# Select Option 10: Test Endpoints
```

#### Scenario 4: Saving Costs

```bash
# Pause the system when not in use
./manage-lumina.sh
# Select Option 5: Pause System

# Resume when needed
./manage-lumina.sh
# Select Option 6: Resume System
```

### Troubleshooting

#### âŒ Frontend shows "Failed to load resource: 404" or "405 Method Not Allowed"

**Problem**: Frontend can't reach the backend API.

**Solution**:
```bash
./manage-lumina.sh
# Select Option 13: Fix Frontend Issues
# Then select Option 5: Fix All Issues
```

This will:
1. Update `.env.production` with correct backend URL
2. Rebuild the frontend
3. Redeploy to S3

#### âŒ Backend services not starting

**Problem**: Docker containers fail to start or crash immediately.

**Solution**: Check the logs:
```bash
./manage-lumina.sh
# Select Option 12: View Docker Logs
```

Common causes:
- **Missing API keys**: Edit `.env` file and add `NVIDIA_API_KEY` and `GOOGLE_GEMINI_API_KEY`
- **Port conflicts**: Another service is using ports 8000, 8001, or 8002
- **Docker build failed**: Check if you have enough disk space

**Fix**:
```bash
# SSH to instance
ssh -i <your-key>.pem ec2-user@<your-ip>

# Edit .env file
nano .env

# Add your API keys, then restart
docker compose -f docker-compose-backend.yml restart
```

#### âŒ "No columns could be queried successfully"

**Problem**: Query service can't find indexes created by extraction service.

**Cause**: Services aren't sharing the `indexed_volume` properly.

**Solution**: Redeploy with the latest code (this was fixed):
```bash
./manage-lumina.sh
# Select Option 8: Redeploy Backend Code
```

#### âŒ "Index 'column_education_history' not found"

**Problem**: Dynamically extracted columns can't be queried.

**Cause**: Column name sanitization mismatch between services.

**Solution**: This was fixed in the latest version. Redeploy:
```bash
./manage-lumina.sh
# Select Option 8: Redeploy Backend Code
```

#### âŒ Frontend shows `[object Object]` in table cells

**Problem**: Nested objects/arrays not displaying correctly.

**Solution**: This was fixed in the latest version. Redeploy frontend:
```bash
./manage-lumina.sh
# Select Option 9: Redeploy Frontend
```

#### âŒ "Request failed with status code 500"

**Problem**: Backend error during processing.

**Solution**: Check backend logs for details:
```bash
./manage-lumina.sh
# Select Option 12: View Docker Logs
# Select Option 2: View specific container logs
# Enter: api-service (or extraction-service, or query-service)
```

Common causes:
- Invalid API keys
- LLM API rate limits exceeded
- Database connection issues

#### âŒ EC2 instance IP changed after restart

**Problem**: After stopping/starting EC2, the public IP changes.

**Solution**: The frontend needs to be updated with the new IP:
```bash
# The new IP is automatically saved to backend-instance-personal.env
# Just redeploy the frontend:
./manage-lumina.sh
# Select Option 9: Redeploy Frontend
```

#### ğŸ’¡ General Debugging Tips

1. **Check system status first**:
   ```bash
   ./manage-lumina.sh
   # Select Option 4: Show System Status
   ```

2. **Test endpoints**:
   ```bash
   ./manage-lumina.sh
   # Select Option 10: Test Endpoints
   ```

3. **View deployment info**:
   ```bash
   ./manage-lumina.sh
   # Select Option 11: Show Deployment Info
   ```

4. **SSH to instance for manual debugging**:
   ```bash
   ssh -i <your-key>.pem ec2-user@<your-ip>
   
   # Check container status
   docker ps
   
   # View logs
   docker logs api-service
   docker logs extraction-service
   docker logs query-service
   
   # Check disk space
   df -h
   
   # Check memory
   free -h
   ```

### Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         AWS Cloud                            â”‚
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚   S3 Bucket        â”‚         â”‚   EC2 Instance      â”‚    â”‚
â”‚  â”‚   (Frontend)       â”‚         â”‚   (t3.medium)       â”‚    â”‚
â”‚  â”‚                    â”‚         â”‚                     â”‚    â”‚
â”‚  â”‚  React App         â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Docker Compose:    â”‚    â”‚
â”‚  â”‚  (Static Files)    â”‚  API    â”‚  - API Service      â”‚    â”‚
â”‚  â”‚                    â”‚  Calls  â”‚  - Extraction Svc   â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚  - Query Service    â”‚    â”‚
â”‚                                  â”‚  - PostgreSQL       â”‚    â”‚
â”‚                                  â”‚  - Redis            â”‚    â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚                                           â”‚                  â”‚
â”‚                                           â–¼                  â”‚
â”‚                                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚                                  â”‚  Shared Volumes:    â”‚    â”‚
â”‚                                  â”‚  - uploaded_files   â”‚    â”‚
â”‚                                  â”‚  - indexed_volume   â”‚    â”‚
â”‚                                  â”‚  - exports_data     â”‚    â”‚
â”‚                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚                           â”‚
                    â–¼                           â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚ NVIDIA NIM   â”‚          â”‚ Google       â”‚
            â”‚ API          â”‚          â”‚ Gemini API   â”‚
            â”‚ (Embedding,  â”‚          â”‚ (Extraction) â”‚
            â”‚  Reranking)  â”‚          â”‚              â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Cost Breakdown

**Monthly Costs (when running 24/7):**
- EC2 t3.medium: ~$30/month
- S3 hosting: ~$0.50/month
- Data transfer: ~$1-2/month
- **Total: ~$32/month**

**Cost Saving Tips:**
- Pause the EC2 instance when not in use (Option 5)
- Use spot instances for development
- Delete resources when done (Option 14)

### Security Best Practices

1. **Never commit API keys** to version control
2. **Use environment variables** for sensitive data
3. **Restrict EC2 security groups** to your IP only
4. **Rotate API keys** regularly
5. **Enable CloudWatch logs** for monitoring

### Getting API Keys

**NVIDIA API Key:**
1. Go to [NVIDIA NGC](https://catalog.ngc.nvidia.com/)
2. Sign up for a free account
3. Generate an API key from your account settings
4. This key works for embeddings, reranking, and LLM

**Google Gemini API Key:**
1. Go to [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Sign in with your Google account
3. Create a new API key
4. This key is used for the extraction LLM

### Environment Variables Reference

The `.env` file on your EC2 instance contains all configuration. Here's what each variable does:

```bash
# Database Configuration
POSTGRES_PASSWORD=lumina_postgres_password_2025
DATABASE_URL=postgresql+asyncpg://lumina:lumina_postgres_password_2025@postgres:5432/lumina_db
REDIS_URL=redis://redis:6379

# Service URLs (Docker network names)
EXTRACTION_SERVICE_URL=http://extraction-service:8001
QUERY_SERVICE_URL=http://query-service:8002

# Application Configuration
PYTHONPATH=/app
EXPORTS_BUCKET_NAME=lumina-exports-bucket

# âš ï¸ REQUIRED: LLM API Keys
NVIDIA_API_KEY="your-nvidia-api-key-here"           # Get from: https://catalog.ngc.nvidia.com/
GOOGLE_GEMINI_API_KEY="your-google-api-key-here"   # Get from: https://makersuite.google.com/app/apikey

# These reuse the NVIDIA key
NVIDIA_EMBED_API_KEY=$NVIDIA_API_KEY
NVIDIA_RERANK_API_KEY=$NVIDIA_API_KEY
NVIDIA_NIM_API_KEY=$NVIDIA_API_KEY

# LLM Models (can be customized)
GRAPH_LLM_MODEL="nvidia/llama-3.1-nemotron-nano-8b-v1"

# Optional: Neo4j for Knowledge Graphs
NEO4J_URL=neo4j://127.0.0.1:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_neo4j_password
```

### Advanced Configuration

#### Using Custom Models

Edit `lumina-backend/.env`:
```bash
# Use different NVIDIA models
GRAPH_LLM_MODEL="nvidia/llama-3.1-nemotron-70b-instruct"

# Use OpenAI instead
LLM_PROVIDER=openai
OPENAI_API_KEY="your-openai-key"
```

#### Scaling for Production

For production workloads:
1. **Compute**: Use larger EC2 instances (t3.large or t3.xlarge)
2. **Database**: Enable RDS PostgreSQL instead of containerized Postgres
3. **CDN**: Use CloudFront CDN for the S3 frontend
4. **Auto-scaling**: Set up EC2 auto-scaling groups
5. **Monitoring**: Enable CloudWatch alarms for CPU, memory, and errors
6. **Load Balancing**: Add an Application Load Balancer for high availability
7. **Backup**: Enable automated RDS backups and S3 versioning

---

## ğŸ—ºï¸ Roadmap

### Phase 1: Core Intelligence (Current)
- âœ… Multi-agent extraction pipeline
- âœ… GPU-accelerated RAG
- âœ… Dynamic field extraction
- âœ… Knowledge graph construction

### Phase 2: Enhanced Capabilities 
- [ ] **Table Extraction**: Dedicated agent for query generation and extracting data from complex tables
- [ ] **Image Analysis**: Extract data from charts/figures (OCR + VLM)
- [ ] **Custom Validators**: User-defined validation logic


---

## ğŸ¤ Contributing

We welcome contributions! Here's how:



## ğŸŒŸ Recognition

Built by researchers, for researchers who are tired of manual data extraction.

**Core Innovation:**
- Multi-agent architecture where each agent has a single, focused task
- Dynamic schema generation and on-demand field extraction
- GPU-accelerated retrieval with structured evidence for hallucination reduction
- Markdown caching for fast re-extraction without re-upload

**Key Technologies:**
- OpenAI Agents SDK for agentic coordination
- NVIDIA NIM for embedding and reranking and high-quality generation

---

### Architecture Overview

```mermaid
graph TB
    subgraph "Document Analysis Phase"
        A[Upload Documents] -->|Async Processing| B[Document Classifier Agent]
        B -->|Classifications| C[Intention Recommender Agent]
        C -->|Recommended Schema| D{User Review & Edit}
    end
    
    subgraph "Schema Generation Phase"
        D -->|User Intention + Fields| E[Schema Designer Agent]
        E -->|Field Recommendations| F[Pydantic Code Agent]
        F -->|Validate Code| G[Code Validator Tool]
        G -->|Check Structure| H[Structure Validator Tool]
        F -->|Generate Model| I[Extraction Prompt Agent]
        F -->|Generate Model| J[Field Mapping Agent]
    end
    
    subgraph "Parallel Extraction Phase"
        I & J -->|Dynamic Model| K[Data Extraction Pipeline]
        K -->|Fan-Out| L1[Extract Doc 1]
        K -->|Fan-Out| L2[Extract Doc 2]
        K -->|Fan-Out| L3[Extract Doc N]
        
        L1 & L2 & L3 -->|Structured Records| M[Database Storage]
        L1 & L2 & L3 -->|Text Chunks| N[Markdown Cache]
    end
    
    subgraph "RAG Indexing Phase"
        M -->|Records| O[Chunking Strategy]
        O -->|Row-wise Chunks| P[NVIDIA Embedding API]
        O -->|Column Chunks| Q[NVIDIA Embedding API]
        P & Q -->|Vectors| R[GPU Vector Store]
    end
    
    subgraph "Query Processing Phase"
        S[User Query] -->|Natural Language| T[Query Router Agent]
        
        T -->|rag_row_wise| U1[Row Retrieval]
        T -->|rag_column_wise| U2[Column Retrieval]
        T -->|dynamic_extraction| U3[Dynamic Extractor]
        T -->|function| U4[Function Handler]
        
        U1 & U2 -->|Embed Query| V[NVIDIA Embedding]
        V -->|GPU Cosine Similarity| W[Top-K Retrieval]
        W -->|Rerank| X[NVIDIA Reranker]
        X -->|Context| Y[Answer Synthesizer]
        
        U3 -->|New Field| Z1[Schema Designer]
        Z1 -->|Extract| Z2[Parallel Extraction]
        Z2 -->|Updated Records| M
        
        U4 -->|Export CSV| AA[File Export Tool]
        
        Y -->|LLM Call| AB[Gemini Generator]
        AB -->|Cited Answer| AC[Response + Sources]
    end
    
    subgraph "Optional: Knowledge Graph"
        X -->|Retrieved Chunks| KG1[LangChain Graph Transformer]
        KG1 -->|Nodes & Edges| KG2[Neo4j Database]
        KG2 -->|Graph Query| KG3[Relationship Discovery]
    end
```

<div align="center">

**From Chaos to Clarity**: Where multi-agent AI meets document intelligence to extract the knowledge hidden in your papers.

---

## ğŸ“– Learn More

### For Users
- **[Quick Start Guide](#-quick-start)** - Get running in 5 minutes
- **[Deployment Guide](#-deployment-guide)** - Detailed deployment instructions
- **[Troubleshooting](#troubleshooting)** - Common issues and solutions

### For Developers
- **[Technical Deep Dive](TECHNICAL_DEEP_DIVE.md)** - Architecture, AI integration, and implementation
- **[Project Structure](#ï¸-project-structure)** - Codebase organization
- **[Contributing](#-contributing)** - How to contribute

### For Evaluators
- **[Hackathon Evaluation](kiro-documentation/hackathon-evaluation/)** - Technical analysis and scoring
- **[Competitive Analysis](kiro-documentation/hackathon-evaluation/COMPETITIVE_ANALYSIS.md)** - Market positioning

[â¬† back to top](#-lumina)

</div>
